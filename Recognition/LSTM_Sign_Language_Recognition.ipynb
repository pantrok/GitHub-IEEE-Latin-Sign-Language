{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMMJciwXmDLz"
      },
      "source": [
        "# Connection to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2QoRQ2lmB0t",
        "outputId": "49c2fc17-7aea-4993-84e3-2c718d4b7056"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rX7whrERICw"
      },
      "source": [
        "#Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67ZkBeisRKXQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
        "from imblearn.under_sampling import AllKNN\n",
        "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
        "from imblearn.under_sampling import OneSidedSelection\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "def create_all_labels(df):\n",
        "    #Create a set to avoid redudant values\n",
        "    all_labels = []\n",
        "    for index, row in df.iterrows():\n",
        "      if not row['label'] in all_labels:\n",
        "        all_labels.append(row['label'])\n",
        "    return all_labels\n",
        "\n",
        "\n",
        "#Return \n",
        "def split_train_test_data(df, df_test_noaug = False, test_split_size = 0.3, scale = False, pca_value = None, undersampling_method = 'OneSide', tsne = None):\n",
        "    \n",
        "    #Get X and Y from the dataframe\n",
        "    X = df.drop(\"label\", axis=1) # Create a data with all columns except labels\n",
        "    y = df[[\"label\"]] # Create the labels dataset\n",
        "    #print(y)\n",
        "\n",
        "    if tsne != None:\n",
        "      X_embedded = TSNE(n_components=tsne, init=\"pca\", learning_rate=\"auto\").fit_transform(X.to_numpy())\n",
        "      for i in range(0,tsne):\n",
        "        df[\"tsne_component_\"+str(i)]=X_embedded[:,i]\n",
        "\n",
        "    if undersampling_method == 'OneSide':\n",
        "      us_method = OneSidedSelection(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Tomek':\n",
        "      us_method = TomekLinks(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Random':\n",
        "      us_method = RandomUnderSampler(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NearMiss':\n",
        "      us_method = NearMiss(version=1, sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'RepeatedEdited':\n",
        "      us_method = RepeatedEditedNearestNeighbours(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'AllKNN':\n",
        "      us_method = AllKNN(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NCleaning':\n",
        "      us_method = NeighbourhoodCleaningRule(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    if undersampling_method != None:\n",
        "      df = df.iloc[us_method.sample_indices_.tolist(),:]\n",
        "      y = df[[\"label\"]]\n",
        "    \n",
        "    if df_test_noaug == True:\n",
        "      test_original_indexes = []\n",
        "      test_total_indexes = math.floor(df.shape[0] * 0.3)\n",
        "      while len(test_original_indexes) < test_total_indexes:\n",
        "        odd_idx = random.randrange(0, df.shape[0], 2)\n",
        "        if not odd_idx in test_original_indexes:\n",
        "          test_original_indexes.append(odd_idx)\n",
        "\n",
        "      rest_indexes = [i for i in range(0, df.shape[0])]\n",
        "      for i in test_original_indexes:\n",
        "        rest_indexes.remove(i)\n",
        "      df_test = df.iloc[test_original_indexes]\n",
        "      df_train = df.iloc[rest_indexes]\n",
        "    #mm = MinMaxScaler()\n",
        "    ss = StandardScaler()\n",
        "    #Split the data, \n",
        "    #train_df, test_df = train_test_split(df, test_size=test_split_size, random_state=42, stratify=y)\n",
        "    if df_test_noaug == True:\n",
        "      X_train = df_train.drop(\"label\", axis=1)\n",
        "      X_test = df_test.drop(\"label\", axis=1)\n",
        "    else:\n",
        "      train_df, test_df = train_test_split(df, test_size=test_split_size, random_state=42)\n",
        "      # Make an instance of the Model\n",
        "      X_train = train_df.drop(\"label\", axis=1)\n",
        "      X_test = test_df.drop(\"label\", axis=1)\n",
        "\n",
        "    if scale == True:\n",
        "      X_train = ss.fit_transform(X_train)\n",
        "      X_test = ss.fit_transform(X_test)\n",
        "\n",
        "    pca = None\n",
        "    if pca_value != None:\n",
        "      pca = PCA(pca_value)\n",
        "      pca.fit(X_train)\n",
        "      X_train = pca.transform(X_train)\n",
        "      X_test = pca.transform(X_test)  \n",
        "\n",
        "    if df_test_noaug == True:\n",
        "      y_train = df_train[[\"label\"]]\n",
        "      y_test = df_test[[\"label\"]]\n",
        "    else:\n",
        "      y_train = train_df[[\"label\"]]\n",
        "      y_test = test_df[[\"label\"]]\n",
        "\n",
        "    #TODO: Pass the data to tensors\n",
        "    data_xtrain_numpy = X_train.to_numpy() if scale == False else X_train\n",
        "    data_xtest_numpy = X_test.to_numpy() if scale == False else X_test\n",
        "    data_ytrain_numpy = y_train.to_numpy()\n",
        "    data_ytest_numpy = y_test.to_numpy()\n",
        "    \n",
        "    return data_xtrain_numpy, data_xtest_numpy, data_ytrain_numpy, data_ytest_numpy, X_train, X_test, y_train, y_test, pca\n",
        "\n",
        "def split_train_test_data_files(df_train, df_test, scale = False, pca_value = None, undersampling_method = 'OneSide'):\n",
        "    \n",
        "    #Get X and Y from the dataframe\n",
        "    X = df_train.drop(\"label\", axis=1) # Create a data with all columns except labels\n",
        "    y = df_train[[\"label\"]] # Create the labels dataset\n",
        "    #print(y)\n",
        "\n",
        "    if undersampling_method == 'OneSide':\n",
        "      us_method = OneSidedSelection(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Tomek':\n",
        "      us_method = TomekLinks(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Random':\n",
        "      us_method = RandomUnderSampler(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NearMiss':\n",
        "      us_method = NearMiss(version=1, sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'RepeatedEdited':\n",
        "      us_method = RepeatedEditedNearestNeighbours(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'AllKNN':\n",
        "      us_method = AllKNN(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NCleaning':\n",
        "      us_method = NeighbourhoodCleaningRule(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    if undersampling_method != None:\n",
        "      df_train = df_train.iloc[us_method.sample_indices_.tolist(),:]\n",
        "      y = df_train[[\"label\"]]\n",
        "    \n",
        "    ss = StandardScaler()\n",
        "    #Split the data, \n",
        "    # Make an instance of the Model\n",
        "    X_train = df_train.drop(\"label\", axis=1)\n",
        "    X_test = df_test.drop(\"label\", axis=1)\n",
        "\n",
        "    if scale == True:\n",
        "      X_train = ss.fit_transform(X_train)\n",
        "      X_test = ss.fit_transform(X_test)\n",
        "\n",
        "    pca = None\n",
        "    if pca_value != None:\n",
        "      pca = PCA(pca_value)\n",
        "      pca.fit(X_train)\n",
        "      X_train = pca.transform(X_train)\n",
        "      X_test = pca.transform(X_test)  \n",
        "\n",
        "    y_train = df_train[[\"label\"]]\n",
        "    y_test = df_test[[\"label\"]]\n",
        "\n",
        "    #TODO: Pass the data to tensors\n",
        "    data_xtrain_numpy = X_train.to_numpy() if scale == False else X_train\n",
        "    data_xtest_numpy = X_test.to_numpy() if scale == False else X_test\n",
        "    data_ytrain_numpy = y_train.to_numpy()\n",
        "    data_ytest_numpy = y_test.to_numpy()\n",
        "\n",
        "    return data_xtrain_numpy, data_xtest_numpy, data_ytrain_numpy, data_ytest_numpy, X_train, X_test, y_train, y_test, pca\n",
        "\n",
        "def split_train_validation_test_data(df_train, df_validation, df_test, scale = False, pca_value = None, undersampling_method = 'OneSide'):\n",
        "    \n",
        "    #Get X and Y from the dataframe\n",
        "    X = df_train.drop(\"label\", axis=1) # Create a data with all columns except labels\n",
        "    y = df_train[[\"label\"]] # Create the labels dataset\n",
        "    #print(y)\n",
        "\n",
        "    if undersampling_method == 'OneSide':\n",
        "      us_method = OneSidedSelection(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Tomek':\n",
        "      us_method = TomekLinks(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Random':\n",
        "      us_method = RandomUnderSampler(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NearMiss':\n",
        "      us_method = NearMiss(version=1, sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'RepeatedEdited':\n",
        "      us_method = RepeatedEditedNearestNeighbours(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'AllKNN':\n",
        "      us_method = AllKNN(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NCleaning':\n",
        "      us_method = NeighbourhoodCleaningRule(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    if undersampling_method != None:\n",
        "      df_train = df_train.iloc[us_method.sample_indices_.tolist(),:]\n",
        "      y = df_train[[\"label\"]]\n",
        "    \n",
        "    ss = StandardScaler()\n",
        "    #Split the data, \n",
        "    # Make an instance of the Model\n",
        "    X_train = df_train.drop(\"label\", axis=1)\n",
        "    X_validation = df_validation.drop(\"label\", axis=1)\n",
        "    X_test = df_test.drop(\"label\", axis=1)\n",
        "\n",
        "    if scale == True:\n",
        "      X_train = ss.fit_transform(X_train)\n",
        "      X_validation = ss.fit_transform(X_validation)\n",
        "      X_test = ss.fit_transform(X_test)\n",
        "\n",
        "    pca = None\n",
        "    if pca_value != None:\n",
        "      pca = PCA(pca_value)\n",
        "      pca.fit(X_train)\n",
        "      X_train = pca.transform(X_train)\n",
        "      X_validation = pca.transform(X_validation)  \n",
        "      X_test = pca.transform(X_test)  \n",
        "\n",
        "    y_train = df_train[[\"label\"]]\n",
        "    y_validation = df_validation[[\"label\"]]\n",
        "    y_test = df_test[[\"label\"]]\n",
        "\n",
        "    #TODO: Pass the data to tensors\n",
        "    data_xtrain_numpy = X_train.to_numpy() if scale == False else X_train\n",
        "    data_xvalidation_numpy = X_validation.to_numpy() if scale == False else X_validation\n",
        "    data_xtest_numpy = X_test.to_numpy() if scale == False else X_test\n",
        "    data_ytrain_numpy = y_train.to_numpy()\n",
        "    data_yvalidation_numpy = y_validation.to_numpy()\n",
        "    data_ytest_numpy = y_test.to_numpy()\n",
        "\n",
        "    return data_xtrain_numpy, data_xvalidation_numpy, data_xtest_numpy, data_ytrain_numpy, data_yvalidation_numpy, data_ytest_numpy, pca\n",
        "  \n",
        "def generate_features_label_tensors(features, label, all_labels, dtype = torch.double):\n",
        "    features_data_tensor = torch.zeros(1, len(features), dtype=dtype)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_data_tensor[0][i] = feature\n",
        "    label_data_tensor = torch.tensor([all_labels.index(label)], dtype=torch.long)\n",
        "    return features_data_tensor, label_data_tensor\n",
        "\n",
        "def generate_labels_encoding(labels, all_labels):\n",
        "    labels_encoded = []\n",
        "    for label in labels:\n",
        "        labels_encoded.append(all_labels.index(label))\n",
        "    return labels_encoded\n",
        "\n",
        "#word from output util function\n",
        "def words_from_output(output, all_labels):\n",
        "    label_idx = torch.argmax(output).item()\n",
        "    return all_labels[label_idx]\n",
        "\n",
        "def get_label_index(label, all_labels):\n",
        "    try:\n",
        "      return all_labels.index(label)\n",
        "    except ValueError:\n",
        "      return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_itrASGztDJC"
      },
      "source": [
        "#LSTM Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb-bYT-2tGgt"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "# Bidirectional recurrent neural network (many-to-one)\n",
        "class BiLSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, device):\n",
        "        super(BiLSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True, device=self.device)\n",
        "        self.fc = nn.Linear(hidden_size*2, num_classes, device=self.device)  # 2 for bidirection\n",
        "        self.double()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Set initial states\n",
        "        \n",
        "        h0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size, dtype=torch.double).to(self.device) # 2 for bidirection \n",
        "        c0 = torch.zeros(self.num_layers*2, x.size(0), self.hidden_size, dtype=torch.double).to(self.device)\n",
        "        \n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size*2)\n",
        "        \n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ratcowyVCf-k"
      },
      "source": [
        "# Training with cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxCD801Z8xzq"
      },
      "outputs": [],
      "source": [
        "class SaveBestModel:\n",
        "    \"\"\"\n",
        "    Class to save the best model while training. If the current epoch's \n",
        "    validation loss is less than the previous least less, then save the\n",
        "    model state.\n",
        "    \"\"\"\n",
        "    def __init__(self, best_valid_loss=float('inf')):\n",
        "        self.best_valid_loss = best_valid_loss\n",
        "        \n",
        "    def __call__(self, current_valid_loss, epoch, model, optimizer, criterion, directory, fold):\n",
        "        if current_valid_loss < self.best_valid_loss:\n",
        "            self.best_valid_loss = current_valid_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch+1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, directory + 'fold_'+str(fold)+'_best_model.pth')\n",
        "            \n",
        "def save_model(epochs, model, optimizer, criterion, directory, fold):\n",
        "    \"\"\"\n",
        "    Function to save the trained model to disk.\n",
        "    \"\"\"\n",
        "    #print(f\"Saving final model...\")\n",
        "    torch.save({\n",
        "                'epoch': epochs,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': criterion,\n",
        "                }, directory + 'fold_'+str(fold)+'_final_model.pth')\n",
        "\n",
        "def save_plots(train_acc, valid_acc, train_loss, valid_loss, directory, undersampling, fold):\n",
        "    \"\"\"\n",
        "    Function to save the loss and accuracy plots to disk.\n",
        "    \"\"\"\n",
        "    # accuracy plots\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(\n",
        "        train_acc, color='green', linestyle='-', \n",
        "        label='train accuracy'\n",
        "    )\n",
        "    if valid_acc != None:\n",
        "      plt.plot(\n",
        "          valid_acc, color='blue', linestyle='-', \n",
        "          label='validataion accuracy'\n",
        "      )\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title(f'Accuracy, undersampling {undersampling}, fold {fold}')\n",
        "    plt.savefig(directory + 'fold_'+str(fold)+'_accuracy.png')\n",
        "    \n",
        "    # loss plots\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(\n",
        "        train_loss, color='orange', linestyle='-', \n",
        "        label='train loss'\n",
        "    )\n",
        "    if valid_loss != None:\n",
        "      plt.plot(\n",
        "          valid_loss, color='red', linestyle='-', \n",
        "          label='validataion loss'\n",
        "      )\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title(f'Loss, undersampling {undersampling}, fold {fold}')\n",
        "    plt.savefig(directory + 'fold_'+str(fold)+'_loss.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivre3TBpN2S5"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def training(input_size, hidden_size, num_layers, num_classes, device, learning_rate,\n",
        "             data_xtrain_numpy,data_ytrain_numpy,data_xvalidation_numpy,data_yvalidation_numpy, undersampling, fold, all_labels, directory, batch_size=50):\n",
        "  \n",
        "  all_losses = []\n",
        "  all_losses_test = []\n",
        "  train_acc, valid_acc = [], []\n",
        "  scheduler_factor = 0.1\n",
        "  scheduler_patience = 5\n",
        "\n",
        "  model = BiLSTM(input_size, hidden_size, num_layers, num_classes, device)\n",
        "\n",
        "  # Loss and optimizer\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
        "  # initialize SaveBestModel class\n",
        "  save_best_model = SaveBestModel()\n",
        "\n",
        "  # Train the model\n",
        "  N_rows = len(data_xtrain_numpy)\n",
        "  size_step = math.ceil(len(data_xtrain_numpy) / batch_size)\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      model.train(True)\n",
        "      current_loss = 0\n",
        "      train_running_correct = 0\n",
        "      for i in range(0, size_step): \n",
        "          lower_idx = i * batch_size\n",
        "          upper_idx = (i+1) * batch_size if ((i+1) * batch_size) < N_rows else N_rows\n",
        "\n",
        "          features_data_tensor = torch.from_numpy(data_xtrain_numpy[lower_idx:upper_idx]).to(device)\n",
        "          labels_encoded = generate_labels_encoding(data_ytrain_numpy[lower_idx:upper_idx], all_labels)\n",
        "          data_y = data_ytrain_numpy[lower_idx:upper_idx]\n",
        "          labels_data_tensor = torch.from_numpy(np.array(labels_encoded)).to(device)\n",
        "\n",
        "          #reshaping to rows, timestamps, features\n",
        "          X_train_tensors_final = torch.reshape(features_data_tensor,   (features_data_tensor.shape[0], 1, features_data_tensor.shape[1]))\n",
        "          \n",
        "          # Forward pass\n",
        "          outputs = model(X_train_tensors_final)\n",
        "          loss = criterion(outputs, labels_data_tensor)\n",
        "\n",
        "          # Backward and optimize\n",
        "          optimizer.zero_grad()\n",
        "          current_loss += loss.item()\n",
        "\n",
        "          # calculate the accuracy\n",
        "          _, preds = torch.max(outputs.data, 1)\n",
        "          train_running_correct += (preds == labels_data_tensor).sum().item()\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #scheduler.step(current_loss / size_step)\n",
        "      epoch_loss = current_loss / size_step\n",
        "      epoch_acc = 100. * (train_running_correct / len(data_xtrain_numpy))\n",
        "      all_losses.append(epoch_loss)\n",
        "      train_acc.append(epoch_acc)\n",
        "      \n",
        "      if data_xvalidation_numpy != None:\n",
        "        model.train(False)\n",
        "        size_step_test = math.ceil(len(data_xvalidation_numpy) / batch_size)\n",
        "        N_rows_test = len(data_xvalidation_numpy)\n",
        "        running_vloss = 0.0\n",
        "        valid_running_correct = 0\n",
        "        for i in range(0, size_step_test): #hay que hacer esto por batch, la shape tiene que ser (batch, 1 (seq_length), features)\n",
        "            lower_idx = i * batch_size\n",
        "            upper_idx = (i+1) * batch_size if ((i+1) * batch_size) < N_rows_test else N_rows_test\n",
        "            \n",
        "            features_data_tensor = torch.from_numpy(data_xvalidation_numpy[lower_idx:upper_idx]).to(device)\n",
        "            labels_encoded = generate_labels_encoding(data_yvalidation_numpy[lower_idx:upper_idx], all_labels)\n",
        "            data_y = data_yvalidation_numpy[lower_idx:upper_idx]\n",
        "            labels_data_tensor = torch.from_numpy(np.array(labels_encoded)).to(device)\n",
        "\n",
        "            #reshaping to rows, timestamps, features\n",
        "            X_test_tensors_final = torch.reshape(features_data_tensor,   (features_data_tensor.shape[0], 1, features_data_tensor.shape[1]))\n",
        "            #print(f'features shape after reshape {X_train_tensors_final.shape}')\n",
        "\n",
        "            # Forward pass\n",
        "            voutputs = model(X_test_tensors_final)\n",
        "            vloss = criterion(voutputs, labels_data_tensor)\n",
        "            running_vloss += vloss.item()\n",
        "\n",
        "            # calculate the accuracy\n",
        "            _, preds = torch.max(voutputs.data, 1)\n",
        "            valid_running_correct += (preds == labels_data_tensor).sum().item()\n",
        "\n",
        "            #scheduler.step(running_vloss / size_step_test)\n",
        "\n",
        "      if data_xvalidation_numpy != None:\n",
        "        epoch_acc = 100. * (valid_running_correct / len(data_xtrain_numpy))  \n",
        "        valid_acc.append(epoch_acc)\n",
        "        epoch_loss = running_vloss / size_step_test\n",
        "        all_losses_test.append(epoch_loss)\n",
        "      \n",
        "      #all_accuracies.append(test(model, data_xtrain_numpy, data_ytrain_numpy, all_labels, device))\n",
        "      #print(all_accuracies)\n",
        "\n",
        "      save_best_model(epoch_loss, epoch, model, optimizer, loss, directory, fold)\n",
        "  \n",
        "  # save the trained model weights for a final time\n",
        "  save_model(num_epochs, model, optimizer, loss, directory, fold)\n",
        "  # save the loss and accuracy plots\n",
        "  save_plots(train_acc, valid_acc, all_losses, all_losses_test, directory, undersampling, fold)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xVyliyrP8XU"
      },
      "outputs": [],
      "source": [
        "def test(model, data_xtest_numpy, data_ytest_numpy, all_labels, device):\n",
        "  confusion = torch.zeros(len(all_labels), len(all_labels))\n",
        "  correct_guesses = 0\n",
        "\n",
        "  # Go through a bunch of examples and record which are correctly guessed\n",
        "  for i, data_xtest in enumerate(data_xtest_numpy):\n",
        "      with torch.no_grad():\n",
        "          data_ytest = data_ytest_numpy[i]\n",
        "          features_data_tensor, label_data_tensor = generate_features_label_tensors(data_xtest, data_ytest, all_labels)\n",
        "          #Reshape features tensor\n",
        "          X_test_tensors_final = torch.reshape(features_data_tensor, (features_data_tensor.shape[0], 1, features_data_tensor.shape[1])).to(device)\n",
        "          output = model(X_test_tensors_final)\n",
        "          guess = words_from_output(output, all_labels)\n",
        "          #print(guess)\n",
        "          #category_i = np.where(unique_test_elements == data_ytest)[0][0]\n",
        "          category_i = all_labels.index(data_ytest)\n",
        "          #print(np.where(unique_test_elements == guess))\n",
        "          #guess_i = np.where(unique_test_elements == guess)[0][0]\n",
        "          guess_i = all_labels.index(guess)\n",
        "          confusion[category_i][guess_i] += 1\n",
        "          correct_guesses = (correct_guesses + 1) if category_i == guess_i else correct_guesses\n",
        "\n",
        "  accuracy = (correct_guesses/len(data_xtest_numpy))*100\n",
        "  #print(f'Total: {len(data_xtest_numpy)}, Correct: {correct_guesses}')\n",
        "  #print(f'Accuracy validation data: {accuracy}%')\n",
        "  #for i in range(len(all_labels)):\n",
        "  #  print(f'Accuracy label {all_labels[i]} \"({confusion[i].sum()})\" : {(confusion[i][i]/confusion[i].sum())*100}%')\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "def test_last_model(model, checkpoint, data_xtest_numpy, data_ytest_numpy, all_labels, device):\n",
        "    #print('Loading last epoch saved model weights...')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    test_acc = test(model, data_xtest_numpy, data_ytest_numpy, all_labels, device)\n",
        "    #print(f\"Last epoch saved model accuracy: {test_acc:.3f}\")\n",
        "    return test_acc\n",
        "\n",
        "def test_best_model(model, checkpoint, data_xtest_numpy, data_ytest_numpy, all_labels, device):\n",
        "    #print('Loading best epoch saved model weights...')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    test_acc = test(model, data_xtest_numpy, data_ytest_numpy, all_labels, device)\n",
        "    #print(f\"Best epoch saved model accuracy: {test_acc:.3f}\")\n",
        "    return test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "w_BcIMZkbUD5",
        "outputId": "6cb733fc-2864-4076-9785-166ce94111ef"
      },
      "outputs": [
      ],
      "source": [
        "import random\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import datetime\n",
        "from pathlib import Path\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "#sequence_length = 28\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "batch_size = 50\n",
        "num_epochs = 50\n",
        "learning_rate = 0.003\n",
        "k_folds = 3\n",
        "\n",
        "# Arrays to store the results\n",
        "video_str = []\n",
        "fold_1_best, fold_1_last = [], []\n",
        "fold_2_best, fold_2_last = [], []\n",
        "fold_3_best, fold_3_last = [], []\n",
        "us_arr = []\n",
        "\n",
        "# assign directory\n",
        "directory = '/content/gdrive/My Drive/your_directory/LIBRAS Data/'\n",
        "now = datetime.datetime.now()\n",
        "experiment_directory = '/content/gdrive/My Drive/your_directory/LIBRAS Data/Experiments/' + str(now)\n",
        "Path(experiment_directory).mkdir(parents=True, exist_ok=True)\n",
        "Path(experiment_directory + '/outputs/').mkdir(parents=True, exist_ok=True)\n",
        " \n",
        "# iterate over files in that directory\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    # checking if it is a file\n",
        "    if os.path.isfile(f) and ('All_Vocab.xlsx' in f):\n",
        "        df = pd.read_excel(f, index_col=0)\n",
        "        \n",
        "        #Generate labels\n",
        "        all_labels = create_all_labels(df[[\"label\"]])\n",
        "\n",
        "        undersampling_methods = ['OneSide', 'Tomek', 'AllKNN', None]\n",
        "        for undersampling_method in undersampling_methods:\n",
        "          undersampling_directory = experiment_directory + \"/outputs/\" + undersampling_method + \"/\" if undersampling_method != None else experiment_directory + \"/outputs/None/\"\n",
        "          Path(undersampling_directory).mkdir(parents=True, exist_ok=True)\n",
        "          video_str.append(f)\n",
        "          us_arr.append(undersampling_method)\n",
        "          for fold in range(k_folds):\n",
        "            #print(f'Undersampling method: {undersampling_method}, Fold: {(int(fold)+1)}')\n",
        "            data_xtrain_numpy, data_xtest_numpy, data_ytrain_numpy, data_ytest_numpy, X_train, X_test, y_train, y_test, pca = split_train_test_data(df, df_test_noaug=True, test_split_size = 0.3, scale = True, pca_value = 0.95, undersampling_method = undersampling_method, tsne = None)\n",
        "\n",
        "            N_features = len(data_xtrain_numpy[0])\n",
        "            input_size = N_features #Tamaño del numero de caracteristicas\n",
        "            num_classes = len(all_labels) #Aqui es el numero de labels\n",
        "            \n",
        "            #Model Training\n",
        "            \n",
        "            model = training(input_size, hidden_size, num_layers, num_classes, device, learning_rate,data_xtrain_numpy,data_ytrain_numpy,None,None,undersampling_method, (fold+1), all_labels, undersampling_directory)\n",
        "\n",
        "            #Testing Model\n",
        "            best_model_cp = torch.load(undersampling_directory + 'fold_'+str(fold+1)+'_best_model.pth')\n",
        "            best_model_epoch = best_model_cp['epoch']\n",
        "            print(f\"Best model was saved at {best_model_epoch} epochs\\n\")\n",
        "            # load the last model checkpoint\n",
        "            last_model_cp = torch.load(undersampling_directory + 'fold_'+str(fold+1)+'_final_model.pth')\n",
        "            last_model_epoch = last_model_cp['epoch']\n",
        "            print(f\"Last model was saved at {last_model_epoch} epochs\\n\")\n",
        "            accuracy_last = test_last_model(model, last_model_cp, data_xtest_numpy, data_ytest_numpy, all_labels, device)\n",
        "            accuracy_best = test_best_model(model, best_model_cp, data_xtest_numpy, data_ytest_numpy, all_labels, device)\n",
        "            if fold == 0:\n",
        "              fold_1_last.append(accuracy_last)\n",
        "              fold_1_best.append(accuracy_best)\n",
        "            elif fold == 1:\n",
        "              fold_2_last.append(accuracy_last)\n",
        "              fold_2_best.append(accuracy_best)\n",
        "            elif fold == 2:\n",
        "              fold_3_last.append(accuracy_last)\n",
        "              fold_3_best.append(accuracy_best)\n",
        "\n",
        "df_save_results = pd.DataFrame()\n",
        "# append columns to an empty DataFrame\n",
        "df_save_results['Video'] = video_str\n",
        "df_save_results['Undersampling'] = us_arr\n",
        "df_save_results['Fold 1 Best'] = fold_1_best\n",
        "df_save_results['Fold 1 Last'] = fold_1_last\n",
        "df_save_results['Fold 2 Best'] = fold_2_best\n",
        "df_save_results['Fold 2 Last'] = fold_2_last\n",
        "df_save_results['Fold 3 Best'] = fold_3_best\n",
        "df_save_results['Fold 3 Last'] = fold_3_last\n",
        "\n",
        "df_save_results.to_excel(experiment_directory + \"/All_results_bilstm_3folds_50epochs.xlsx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
