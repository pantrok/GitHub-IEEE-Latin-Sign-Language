{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Google Drive Connection"
      ],
      "metadata": {
        "id": "Jkt1W4PA7cxV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvS4tRR_7ICw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preparation"
      ],
      "metadata": {
        "id": "aEwAotu9kOjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.under_sampling import RepeatedEditedNearestNeighbours\n",
        "from imblearn.under_sampling import AllKNN\n",
        "from imblearn.under_sampling import NeighbourhoodCleaningRule\n",
        "from imblearn.under_sampling import OneSidedSelection\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "\n",
        "def create_all_labels(y):\n",
        "    #Create a set to avoid redudant values\n",
        "    all_labels = []\n",
        "    for index, row in df.iterrows():\n",
        "      if not row['label'] in all_labels:\n",
        "        all_labels.append(row['label'])\n",
        "    return all_labels\n",
        "\n",
        "\n",
        "#Return \n",
        "def split_train_test_data(df, all_labels, df_test_noaug = False, test_split_size = 0.3, scale = False, pca_value = None, undersampling_method = 'OneSide', tsne = None):\n",
        "    \n",
        "    #Get X and Y from the dataframe\n",
        "    X = df.drop(\"label\", axis=1) # Create a data with all columns except labels\n",
        "    y = df[[\"label\"]] # Create the labels dataset\n",
        "\n",
        "    if tsne != None:\n",
        "      X_embedded = TSNE(n_components=tsne, init=\"pca\", learning_rate=\"auto\").fit_transform(X.to_numpy())\n",
        "      for i in range(0,tsne):\n",
        "        df[\"tsne_component_\"+str(i)]=X_embedded[:,i]\n",
        "\n",
        "    if undersampling_method == 'OneSide':\n",
        "      us_method = OneSidedSelection(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Tomek':\n",
        "      us_method = TomekLinks(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'Random':\n",
        "      us_method = RandomUnderSampler(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NearMiss':\n",
        "      us_method = NearMiss(version=1, sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'RepeatedEdited':\n",
        "      us_method = RepeatedEditedNearestNeighbours(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'AllKNN':\n",
        "      us_method = AllKNN(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    elif undersampling_method == 'NCleaning':\n",
        "      us_method = NeighbourhoodCleaningRule(sampling_strategy='majority')\n",
        "      X_us, y_us = us_method.fit_resample(X, y)\n",
        "    if undersampling_method != None:\n",
        "      df = df.iloc[us_method.sample_indices_.tolist(),:]\n",
        "      y = df[[\"label\"]]\n",
        "    \n",
        "    if df_test_noaug == True:\n",
        "      test_original_indexes = []\n",
        "      test_total_indexes = math.floor(df.shape[0] * 0.3)\n",
        "      while len(test_original_indexes) < test_total_indexes:\n",
        "        odd_idx = random.randrange(0, df.shape[0], 2)\n",
        "        if not odd_idx in test_original_indexes:\n",
        "          test_original_indexes.append(odd_idx)\n",
        "\n",
        "      rest_indexes = [i for i in range(0, df.shape[0])]\n",
        "      for i in test_original_indexes:\n",
        "        rest_indexes.remove(i)\n",
        "      df_test = df.iloc[test_original_indexes]\n",
        "      df_train = df.iloc[rest_indexes]\n",
        "    ss = StandardScaler()\n",
        "    #Split the data, \n",
        "    if df_test_noaug == True:\n",
        "      X_train = df_train.drop(\"label\", axis=1)\n",
        "      X_test = df_test.drop(\"label\", axis=1)\n",
        "    else:\n",
        "      train_df, test_df = train_test_split(df, test_size=test_split_size, random_state=42)\n",
        "      # Make an instance of the Model\n",
        "      X_train = train_df.drop(\"label\", axis=1)\n",
        "      X_test = test_df.drop(\"label\", axis=1)\n",
        "\n",
        "    if scale == True:\n",
        "      X_train = ss.fit_transform(X_train)\n",
        "      X_test = ss.fit_transform(X_test)\n",
        "\n",
        "    pca = None\n",
        "    if pca_value != None:\n",
        "      pca = PCA(pca_value)\n",
        "      pca.fit(X_train)\n",
        "      X_train = pca.transform(X_train)\n",
        "      X_test = pca.transform(X_test)  \n",
        "    \n",
        "    if df_test_noaug == True:\n",
        "      y_train = df_train[[\"label\"]]\n",
        "      y_test = df_test[[\"label\"]]\n",
        "    else:\n",
        "      y_train = train_df[[\"label\"]]\n",
        "      y_test = test_df[[\"label\"]]\n",
        "    \n",
        "    data_xtrain_numpy = X_train.to_numpy() if scale == False else X_train\n",
        "    data_xtest_numpy = X_test.to_numpy() if scale == False else X_test\n",
        "    data_ytrain_numpy = y_train.to_numpy()\n",
        "    data_ytest_numpy = y_test.to_numpy()\n",
        "\n",
        "    return data_xtrain_numpy, data_xtest_numpy, data_ytrain_numpy, data_ytest_numpy, X_train, X_test, y_train, y_test, pca\n",
        "  \n",
        "def generate_features_label_tensors(features, label, all_labels, dtype = torch.double):\n",
        "    features_data_tensor = torch.zeros(1, len(features), dtype=dtype)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_data_tensor[0][i] = feature\n",
        "    label_data_tensor = torch.tensor([all_labels.index(label)], dtype=torch.long)\n",
        "    return features_data_tensor, label_data_tensor\n",
        "\n",
        "def generate_labels_encoding(labels, all_labels):\n",
        "    labels_encoded = []\n",
        "    for label in labels:\n",
        "        labels_encoded.append(all_labels.index(label))\n",
        "    return labels_encoded\n",
        "\n",
        "#word from output util function\n",
        "def words_from_output(output, all_labels):\n",
        "    label_idx = torch.argmax(output).item()\n",
        "    return all_labels[label_idx]\n",
        "\n",
        "def get_label_index(label, all_labels):\n",
        "    try:\n",
        "      return all_labels.index(label)\n",
        "    except ValueError:\n",
        "      return None\n",
        "      "
      ],
      "metadata": {
        "id": "MRWs6FVhkRTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformers Classes"
      ],
      "metadata": {
        "id": "fKNLLM_w7kDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import torch\n",
        "\n",
        "import torch.nn as nn\n",
        "from typing import Optional\n",
        "\n",
        "class SPOTERTransformerDecoderLayer(nn.TransformerDecoderLayer):\n",
        "    \"\"\"\n",
        "    Edited TransformerDecoderLayer implementation omitting the redundant self-attention operation as opposed to the\n",
        "    standard implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, d_model, nhead, dim_feedforward, dropout, activation):\n",
        "        super(SPOTERTransformerDecoderLayer, self).__init__(d_model, nhead, dim_feedforward, dropout, activation)\n",
        "\n",
        "        del self.self_attn\n",
        "\n",
        "    def forward(self, tgt: torch.Tensor, memory: torch.Tensor, tgt_mask: Optional[torch.Tensor] = None,\n",
        "                memory_mask: Optional[torch.Tensor] = None, tgt_key_padding_mask: Optional[torch.Tensor] = None,\n",
        "                memory_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "\n",
        "        tgt = tgt + self.dropout1(tgt)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory, attn_mask=memory_mask,\n",
        "                                   key_padding_mask=memory_key_padding_mask)[0]\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        tgt = self.norm3(tgt)\n",
        "\n",
        "        return tgt"
      ],
      "metadata": {
        "id": "QuBSbZUbouV8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _get_clones(mod, n):\n",
        "    return nn.ModuleList([copy.deepcopy(mod) for _ in range(n)])"
      ],
      "metadata": {
        "id": "kpYx6o_w8SOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SPOTER(nn.Module):\n",
        "    \"\"\"\n",
        "    Implementation of the SPOTER (Sign POse-based TransformER) architecture for sign language recognition from sequence\n",
        "    of skeletal data.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes, num_heads, hidden_dim=55):\n",
        "        super().__init__()\n",
        "\n",
        "        self.row_embed = nn.Parameter(torch.rand(50, hidden_dim))\n",
        "        self.pos = nn.Parameter(torch.cat([self.row_embed[0].unsqueeze(0).repeat(1, 1, 1)], dim=-1).flatten(0, 1).unsqueeze(0))\n",
        "        self.class_query = nn.Parameter(torch.rand(1, hidden_dim))\n",
        "        self.transformer = nn.Transformer(hidden_dim, num_heads, 6, 6)\n",
        "        self.linear_class = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "        # Deactivate the initial attention decoder mechanism\n",
        "        custom_decoder_layer = SPOTERTransformerDecoderLayer(self.transformer.d_model, self.transformer.nhead, 2048,\n",
        "                                                             0.1, \"relu\")\n",
        "        self.transformer.decoder.layers = _get_clones(custom_decoder_layer, self.transformer.decoder.num_layers)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        h = torch.unsqueeze(inputs.flatten(start_dim=1), 1).float()\n",
        "        h = self.transformer(self.pos + h, self.class_query.unsqueeze(0)).transpose(0, 1)\n",
        "        res = self.linear_class(h)\n",
        "\n",
        "        return res"
      ],
      "metadata": {
        "id": "Q-FqY85IPXIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class GaussianNoise(object):\n",
        "    def __init__(self, mean=0., std=1.):\n",
        "        self.std = std\n",
        "        self.mean = mean\n",
        "\n",
        "    def __call__(self, tensor):\n",
        "        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)"
      ],
      "metadata": {
        "id": "vuU3UPfLKtbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "import torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch.utils.data as torch_data\n",
        "import numpy as np\n",
        "\n",
        "from random import randrange\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def create_all_labels(df):\n",
        "    #Create a set to avoid redudant values\n",
        "    all_labels = []\n",
        "    for index, row in df.iterrows():\n",
        "      if not row['label'] in all_labels:\n",
        "        all_labels.append(row['label'])\n",
        "    return all_labels\n",
        "\n",
        "def generate_labels_encoding(labels, all_labels):\n",
        "    labels_encoded = []\n",
        "    for label in labels:\n",
        "        labels_encoded.append(all_labels.index(label))\n",
        "    return labels_encoded\n",
        "\n",
        "def load_dataset(file_location: str):\n",
        "    \n",
        "    df = pd.read_excel(file_location, index_col=0)\n",
        "    \n",
        "    all_labels = create_all_labels(df)\n",
        "    labels_encoded = generate_labels_encoding(df[\"label\"].to_list(), all_labels)\n",
        "    \n",
        "    labels = [label + 1 for label in labels_encoded]\n",
        "    data = []\n",
        "\n",
        "    X = df.drop(\"label\", axis = 1)\n",
        "    num_features = X.shape[1]\n",
        "    for row in range(0, X.shape[0]):\n",
        "      print(ast.literal_eval(X.iloc[row].values[0]))\n",
        "      current_row = np.empty(shape=(len(ast.literal_eval(X.iloc[row].values[0])), num_features))\n",
        "      for column in range(len(X.iloc[row].values)):\n",
        "        current_row[:,column] = ast.literal_eval(X.iloc[row].values[column])\n",
        "      data.append(current_row)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "class CzechSLRDataset(torch_data.Dataset):\n",
        "    \"\"\"Advanced object representation of the HPOES dataset for loading hand joints landmarks utilizing the Torch's\n",
        "    built-in Dataset properties\"\"\"\n",
        "\n",
        "    data: [np.ndarray]\n",
        "    labels: [np.ndarray]\n",
        "\n",
        "    def __init__(self, dataset_filename: str, num_labels=5, transform=None, augmentations=False,\n",
        "                 augmentations_prob=0.5, normalize=True):\n",
        "        \"\"\"\n",
        "        Initiates the HPOESDataset with the pre-loaded data from the h5 file.\n",
        "\n",
        "        :param dataset_filename: Path to the h5 file\n",
        "        :param transform: Any data transformation to be applied (default: None)\n",
        "        \"\"\"\n",
        "\n",
        "        loaded_data = load_dataset(dataset_filename)\n",
        "        data, labels = loaded_data[0], loaded_data[1]\n",
        "        #print(labels)\n",
        "\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "        self.targets = list(labels)\n",
        "        self.num_labels = num_labels\n",
        "        self.transform = transform\n",
        "\n",
        "        self.augmentations = augmentations\n",
        "        self.augmentations_prob = augmentations_prob\n",
        "        self.normalize = normalize\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Allocates, potentially transforms and returns the item at the desired index.\n",
        "\n",
        "        :param idx: Index of the item\n",
        "        :return: Tuple containing both the depth map and the label\n",
        "        \"\"\"\n",
        "\n",
        "        depth_map = torch.from_numpy(np.copy(self.data[idx]))\n",
        "        label = torch.Tensor([self.labels[idx] - 1])\n",
        "\n",
        "        if self.transform:\n",
        "            depth_map = self.transform(depth_map)\n",
        "\n",
        "        return depth_map, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pass"
      ],
      "metadata": {
        "id": "KGN-XGn-Phue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utils"
      ],
      "metadata": {
        "id": "BAOlMWu2QESO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import torch\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device, scheduler=None):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.squeeze(0).to(device)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "\n",
        "        loss = criterion(outputs[0], labels[0])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss\n",
        "\n",
        "        # Statistics\n",
        "        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n",
        "            pred_correct += 1\n",
        "        pred_all += 1\n",
        "\n",
        "    if scheduler:\n",
        "        scheduler.step(running_loss.item() / len(dataloader))\n",
        "\n",
        "    return running_loss, pred_correct, pred_all, (pred_correct / pred_all)\n",
        "\n",
        "\n",
        "def evaluate(model, num_classes, dataloader, device, print_stats=False):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "    stats = {i: [0, 0] for i in range(num_classes)}\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.squeeze(0).to(device)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "\n",
        "        # Statistics\n",
        "        if int(torch.argmax(torch.nn.functional.softmax(outputs, dim=2))) == int(labels[0][0]):\n",
        "            stats[int(labels[0][0])][0] += 1\n",
        "            pred_correct += 1\n",
        "\n",
        "        stats[int(labels[0][0])][1] += 1\n",
        "        pred_all += 1\n",
        "\n",
        "    if print_stats:\n",
        "        stats = {key: value[0] / value[1] for key, value in stats.items() if value[1] != 0}\n",
        "        print(\"Label accuracies statistics:\")\n",
        "        print(str(stats) + \"\\n\")\n",
        "        logging.info(\"Label accuracies statistics:\")\n",
        "        logging.info(str(stats) + \"\\n\")\n",
        "\n",
        "    return pred_correct, pred_all, (pred_correct / pred_all)\n",
        "\n",
        "\n",
        "def evaluate_top_k(model, dataloader, device, k=5):\n",
        "\n",
        "    pred_correct, pred_all = 0, 0\n",
        "\n",
        "    for i, data in enumerate(dataloader):\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.squeeze(0).to(device)\n",
        "        labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(inputs).expand(1, -1, -1)\n",
        "\n",
        "        if int(labels[0][0]) in torch.topk(outputs, k).indices.tolist():\n",
        "            pred_correct += 1\n",
        "\n",
        "        pred_all += 1\n",
        "\n",
        "    return pred_correct, pred_all, (pred_correct / pred_all)\n"
      ],
      "metadata": {
        "id": "-HntxU_FO1Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import Counter\n",
        "from torch.utils.data import Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def __balance_val_split(dataset, val_split=0.):\n",
        "    targets = np.array(dataset.targets)\n",
        "    train_indices, val_indices = train_test_split(\n",
        "        np.arange(targets.shape[0]),\n",
        "        test_size=val_split,\n",
        "        stratify=targets\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(dataset, indices=train_indices)\n",
        "    val_dataset = Subset(dataset, indices=val_indices)\n",
        "\n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "def __split_of_train_sequence(subset: Subset, train_split=1.0):\n",
        "    if train_split == 1:\n",
        "        return subset\n",
        "\n",
        "    targets = np.array([subset.dataset.targets[i] for i in subset.indices])\n",
        "    train_indices, _ = train_test_split(\n",
        "        np.arange(targets.shape[0]),\n",
        "        test_size=1 - train_split,\n",
        "        stratify=targets\n",
        "    )\n",
        "\n",
        "    train_dataset = Subset(subset.dataset, indices=[subset.indices[i] for i in train_indices])\n",
        "\n",
        "    return train_dataset\n",
        "\n",
        "\n",
        "def __log_class_statistics(subset: Subset):\n",
        "    train_classes = [subset.dataset.targets[i] for i in subset.indices]\n",
        "    print(dict(Counter(train_classes)))\n"
      ],
      "metadata": {
        "id": "0-Uq-CrfRH0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "rlNR0d9dQGSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "\n",
        "def train():\n",
        "\n",
        "    # MARK: TRAINING PREPARATION AND MODULES\n",
        "\n",
        "    # Initialize all the random seeds\n",
        "    random.seed(379)\n",
        "    np.random.seed(379)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(379)\n",
        "    os.environ[\"CUDA_LAUNCH_BLOCKING\"] = str(1)\n",
        "    torch.manual_seed(379)\n",
        "    torch.cuda.manual_seed(379)\n",
        "    torch.cuda.manual_seed_all(379)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    g = torch.Generator()\n",
        "    g.manual_seed(379)\n",
        "\n",
        "    # Set device to CUDA only if applicable\n",
        "    device = torch.device(\"cpu\")\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "\n",
        "\t  # Training set\n",
        "    transform = transforms.Compose([GaussianNoise(0, 0.001)])\n",
        "    train_set = CzechSLRDataset('/content/gdrive/My Drive/your_directory/LIBRAS Data/All_Vocab_Transformer_17102022.xlsx', transform=transform)\n",
        "\t\n",
        "    \n",
        "    # Construct the model\n",
        "    num_classes = len(np.unique(np.array(train_set.labels)))\n",
        "    hidden_dim = 22\n",
        "    lr = 0.001 #original 0.001\n",
        "    scheduler_factor = 0.1\n",
        "    scheduler_patience = 5\n",
        "    num_heads = 11\n",
        "    slrt_model = SPOTER(num_classes=num_classes, num_heads=num_heads, hidden_dim=hidden_dim)\n",
        "    slrt_model.train(True)\n",
        "    slrt_model.to(device)\n",
        "\n",
        "    # Construct the other modules\n",
        "    cel_criterion = nn.CrossEntropyLoss()\n",
        "    sgd_optimizer = optim.SGD(slrt_model.parameters(), lr=lr)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(sgd_optimizer, factor=scheduler_factor, patience=scheduler_patience, verbose=True)\n",
        "\n",
        "    # Ensure that the path for checkpointing and for images both exist\n",
        "    experiment_name = 'LIBRAS__' + str(time.time())\n",
        "    Path(\"/content/gdrive/My Drive/your_directory/LIBRAS Data/Experiments/out-checkpoints/\" + experiment_name + \"/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(\"/content/gdrive/My Drive/your_directory/LIBRAS Data/Experiments/out-img/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # MARK: DATA\n",
        "\n",
        "    train_set, eval_set = __balance_val_split(train_set, 0.3)\n",
        "    train_loader = DataLoader(train_set, shuffle=True, generator=g)\n",
        "    eval_loader = DataLoader(eval_set, shuffle=True, generator=g)\n",
        "\n",
        "    # MARK: TRAINING\n",
        "    train_acc, val_acc = 0, 0\n",
        "    losses, train_accs, val_accs = [], [], []\n",
        "    lr_progress = []\n",
        "    top_train_acc, top_val_acc = 0, 0\n",
        "    checkpoint_index = 0\n",
        "\n",
        "    print(\"Starting \" + experiment_name + \"...\\n\\n\")\n",
        "    epochs = 150\n",
        "    save_checkpoints = True\n",
        "    log_freq = 1\n",
        "    for epoch in range(epochs):\n",
        "        train_loss, _, _, train_acc = train_epoch(slrt_model, train_loader, cel_criterion, sgd_optimizer, device, scheduler)\n",
        "        losses.append(train_loss.item() / len(train_loader))\n",
        "        train_accs.append(train_acc)\n",
        "\n",
        "         # Save checkpoints if they are best in the current subset\n",
        "        if save_checkpoints:\n",
        "            if train_acc > top_train_acc:\n",
        "                top_train_acc = train_acc\n",
        "                torch.save(slrt_model.state_dict(), \"/content/gdrive/My Drive/Doctorado/Programa Tesis/Avances Otoño 2022/out-checkpoints/\" + experiment_name + \"/checkpoint_t_\" + str(checkpoint_index) + \".pth\")\n",
        "\n",
        "        if epoch % log_freq == 0:\n",
        "            print(\"[\" + str(epoch + 1) + \"] TRAIN  loss: \" + str(train_loss.item() / len(train_loader)) + \" acc: \" + str(train_acc))\n",
        "\n",
        "            print(\"\")\n",
        "\n",
        "        # Reset the top accuracies on static subsets\n",
        "        if epoch % 10 == 0:\n",
        "            top_train_acc, top_val_acc = 0, 0\n",
        "            checkpoint_index += 1\n",
        "\n",
        "        lr_progress.append(sgd_optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    # MARK: TESTING\n",
        "\n",
        "    print(\"\\nTesting checkpointed models starting...\\n\")\n",
        "\n",
        "    top_result, top_result_name = 0, \"\"\n",
        "    top_k_result, top_k_result_name = 0, \"\"\n",
        "\n",
        "    if eval_loader:\n",
        "        for i in range(checkpoint_index + 1):\n",
        "            for checkpoint_id in [\"t\"]:\n",
        "                tested_model = SPOTER(num_classes=num_classes, num_heads=num_heads, hidden_dim=hidden_dim)\n",
        "                tested_model.to(device)\n",
        "                tested_model.load_state_dict(torch.load(\"/content/gdrive/My Drive/Doctorado/Programa Tesis/Avances Otoño 2022/out-checkpoints/\" + experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i) + \".pth\"))\n",
        "                tested_model.train(False)\n",
        "                _, _, eval_acc = evaluate(tested_model, num_classes, eval_loader, device, print_stats=True)\n",
        "                pred_correct, pred_all, eval_top_acc = evaluate_top_k(tested_model, eval_loader, device)\n",
        "\n",
        "                if eval_acc > top_result:\n",
        "                    top_result = eval_acc\n",
        "                    top_result_name = experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i)\n",
        "                \n",
        "                if eval_top_acc > top_k_result:\n",
        "                    top_k_result = eval_top_acc\n",
        "                    top_k_result_name = experiment_name + \"/checkpoint_\" + checkpoint_id + \"_\" + str(i)\n",
        "\n",
        "                print(\"checkpoint_\" + checkpoint_id + \"_\" + str(i) + \"  ->  \" + str(eval_acc))\n",
        "\n",
        "        print(\"\\nThe top result was recorded at \" + str(top_result) + \" testing accuracy. The best checkpoint is \" + top_result_name + \".\")\n",
        "        logging.info(\"\\nThe top result was recorded at \" + str(top_result) + \" testing accuracy. The best checkpoint is \" + top_result_name + \".\")\n",
        "\n",
        "        print(\"\\nThe top k result was recorded at \" + str(top_k_result) + \" testing accuracy. The best checkpoint is \" + top_k_result_name + \".\")\n",
        "        logging.info(\"\\nThe top result was recorded at \" + str(top_k_result) + \" testing accuracy. The best checkpoint is \" + top_k_result_name + \".\")\n",
        "\n",
        "\n",
        "    # PLOT 0: Performance (loss, accuracies) chart plotting\n",
        "    if True:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.plot(range(1, len(losses) + 1), losses, c=\"#D64436\", label=\"Training loss\")\n",
        "        ax.plot(range(1, len(train_accs) + 1), train_accs, c=\"#00B09B\", label=\"Training accuracy\")\n",
        "\n",
        "        ax.xaxis.set_major_locator(ticker.MaxNLocator(integer=True))\n",
        "\n",
        "        ax.set(xlabel=\"Epoch\", ylabel=\"Accuracy / Loss\", title=\"\")\n",
        "        plt.legend()\n",
        "        ax.grid()\n",
        "\n",
        "        fig.savefig(\"/content/gdrive/My Drive/Doctorado/Programa Tesis/Avances Otoño 2022/out-img/\" + experiment_name + \"_loss.eps\")\n",
        "\n",
        "    # PLOT 1: Learning rate progress\n",
        "    if True:\n",
        "        fig1, ax1 = plt.subplots()\n",
        "        ax1.plot(range(1, len(lr_progress) + 1), lr_progress, label=\"LR\")\n",
        "        ax1.set(xlabel=\"Epoch\", ylabel=\"LR\", title=\"\")\n",
        "        ax1.grid()\n",
        "\n",
        "        fig1.savefig(\"/content/gdrive/My Drive/Doctorado/Programa Tesis/Avances Otoño 2022/out-img/\" + experiment_name + \"_lr.eps\", format='eps')\n",
        "\n",
        "    print(\"\\nAny desired statistics have been plotted.\\nThe experiment is finished.\")\n",
        "    logging.info(\"\\nAny desired statistics have been plotted.\\nThe experiment is finished.\")\n",
        "    df_graphs_data = pd.DataFrame.from_dict({\n",
        "        'losses': losses, \n",
        "        'train_accs': train_accs, \n",
        "        'lr_progress': lr_progress\n",
        "    })\n",
        "    df_graphs_data.to_csv(\"/content/gdrive/My Drive/Doctorado/Programa Tesis/Avances Otoño 2022/out-img/\" + experiment_name + '_data.csv')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()\n"
      ],
      "metadata": {
        "id": "NEvfTkV5QC1Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}